{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2+VS+CNN & s2+PB+LSTM & 0.8398          & 0.8222          & 0.8305          & 0.3977          & 0.7157          & 0.5116         & 0.5779          \\\\\n",
    "s2+VS+CNN & s3+PB+LSTM & 0.8398          & 0.8222          & 0.8305          & 0.3978          & 0.7157          & 0.5117         & 0.5779          \\\\\n",
    "s3+VS+CNN & s2+PB+LSTM & 0.8395          & 0.8223          & 0.8304          & 0.3982          & 0.7146          & 0.5108         & 0.5778          \\\\\n",
    "s3+VS+CNN & s3+PB+LSTM & 0.8395          & 0.8223          & 0.8304          & 0.3983          & 0.7146          & 0.5109         & 0.5779          \\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s1+vs+lstm+1 & s2+distil+cnn+1\n",
    "s1+vs+lstm+5 & s2+viso+lstm+5\n",
    "s1+vs+lstm+1 & s2+viso+lstm+5\n",
    "s1+vs+lstm+5 & s2+distil+cnn+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task_3(model, tokenizer, dataloader, target_len=512, device='cpu'):\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Wrap the dataloader with tqdm to monitor progress\n",
    "        with tqdm(dataloader, desc=\"Evaluation\") as tqdm_loader:\n",
    "            for batch_idx, data in enumerate(tqdm_loader, 0):\n",
    "                tqdm_loader.set_description(f\"Validation, Batch {batch_idx + 1}/{len(dataloader)}\")\n",
    "\n",
    "                ids = data['input']['input_ids'].to(device, dtype=torch.long)\n",
    "                mask = data['input']['attention_mask'].to(device, dtype=torch.long)\n",
    "                y = data['label']['input_ids'].to(device, dtype=torch.long)\n",
    "\n",
    "                generated_ids = model.generate(\n",
    "                    input_ids=ids,\n",
    "                    attention_mask=mask,\n",
    "                    max_length=target_len,\n",
    "                    num_beams=2,\n",
    "                    repetition_penalty=2.5,\n",
    "                    length_penalty=1.0,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "                preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "                target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "\n",
    "                predictions.extend(preds)\n",
    "                references.extend(target)\n",
    "                \n",
    "    return predictions, references\n",
    "\n",
    "def compute_score_task_3(predictions, references):\n",
    "    bertscore_metric = load_metric('bertscore')\n",
    "    bleu_metric = load_metric('bleu')\n",
    "    rouge_metric = load_metric('rouge')\n",
    "\n",
    "    bertscore_result = bertscore_metric.compute(predictions=predictions, references=references, lang=\"vi\")\n",
    "    bertscore_precision = round(np.mean(bertscore_result['precision']), 4)\n",
    "    bertscore_recall = round(np.mean(bertscore_result['recall']), 4)\n",
    "    bertscore_f1 = round(np.mean(bertscore_result['f1']), 4)\n",
    "\n",
    "    bleuscore_result = bleu_metric.compute(\n",
    "        predictions=[pred.split() for pred in predictions],\n",
    "        references=[[ref.split()] for ref in references],\n",
    "    )\n",
    "    bleuscore = round(bleuscore_result['bleu'], 4)\n",
    "    bleu_prec_1 = round(bleuscore_result['precisions'][0])\n",
    "    bleu_prec_2 = round(bleuscore_result['precisions'][1])\n",
    "    bleu_prec_3 = round(bleuscore_result['precisions'][2])\n",
    "    bleu_prec_4 = round(bleuscore_result['precisions'][3])\n",
    "\n",
    "    rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    rouge_1 = round(rouge_result['rouge1'].mid.fmeasure, 4)\n",
    "    rouge_2 = round(rouge_result['rouge2'].mid.fmeasure, 4)\n",
    "    rouge_L = round(rouge_result['rougeL'].mid.fmeasure, 4)\n",
    "    \n",
    "    return (bertscore_precision, bertscore_recall, bertscore_f1), \\\n",
    "           (bleuscore, bleu_prec_1, bleu_prec_2, bleu_prec_3, bleu_prec_4), \\\n",
    "           (rouge_1, rouge_2, rouge_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"E:\\Learning\\Docker_basic\\basic_kafka\\kltn\\experiments\\results\\task3\\prediction\\outputs_lstm-viso-1_cnn-distil-1_bartpho-syllable_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f41c5e6bf644f6957b84b41440caa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/898 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--vinai--bartpho-syllable-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nBartphoTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m###### Model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# \"vinai/bartpho-syllable-base\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# \"vinai/bartpho-word-base\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# \"VietAI/vit5-base\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvinai/bartpho-syllable-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m     10\u001b[0m generation_model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:880\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    878\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m         )\n\u001b[1;32m--> 880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    882\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1475\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1475\u001b[0m requires_backends(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_backends)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1463\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1461\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nBartphoTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "###### Model\n",
    "\n",
    "# \"vinai/bartpho-syllable-base\"\n",
    "# \"vinai/bartpho-word-base\"\n",
    "# \"VietAI/vit5-base\"\n",
    "\n",
    "model = \"vinai/bartpho-syllable-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "generation_model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
    "# generation_model = generation_model.to(device='gpu')\n",
    "# generation_model.load_state_dict(torch.load(r\"E:\\Learning\\Docker_basic\\basic_kafka\\kltn\\experiments\\results\\task3\\vit5-base_2.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
